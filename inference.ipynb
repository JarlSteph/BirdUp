{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e04fc4f",
   "metadata": {},
   "source": [
    "## code that runs inference daily and gets all the feats it needs\n",
    "\n",
    "finally it stores them in predictions hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708de378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-21 14:28:33,718 INFO: Initializing external client\n",
      "2025-12-21 14:28:33,719 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-21 14:28:35,888 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1327256\n",
      "2025-12-21 14:28:36,795 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-12-21 14:28:36,797 INFO: Initializing external client\n",
      "2025-12-21 14:28:36,797 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-21 14:28:38,198 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1327256\n"
     ]
    }
   ],
   "source": [
    "from Features.df_functions import *\n",
    "from Features.inference import *\n",
    "from Models.Bird_percent import *\n",
    "from hsfs.feature import Feature\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "HOPSWORKS_API_KEY = os.getenv(\"HOPSWORKS_API_KEY\")\n",
    "project = hopsworks.login(project=\"BirdUp\", api_key_value=HOPSWORKS_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39237aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Transforms\n",
    "# get the df locally from hopsworks\n",
    "\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSet:\n",
    "    train: pd.DataFrame\n",
    "    val: pd.DataFrame\n",
    "    test: pd.DataFrame\n",
    "\n",
    "    @property\n",
    "    def train_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(self.train.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    @property\n",
    "    def val_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(self.val.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    @property\n",
    "    def test_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(self.test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def get_feature_group_as_df(feature_group_name: str = \"birding\", feature_group_version: int = 1):\n",
    "    fs = project.get_feature_store()\n",
    "    fg = fs.get_feature_group(\n",
    "        name=feature_group_name, version=feature_group_version\n",
    "    )\n",
    "    df = fg.read()\n",
    "    return df\n",
    "\n",
    "def sort_by_date(df:pd.DataFrame, date_col:str='observation_date'):\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values(by=date_col).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def true_false_to_01(df:pd.DataFrame):\n",
    "    # Vectorized conversion is significantly faster than lambda mapping\n",
    "    return df.astype({col: int for col in df.select_dtypes(include='bool').columns})\n",
    "\n",
    "def encode_reigon(df:pd.DataFrame, region_col:str='region', premade_mapping:dict=None):\n",
    "    # calculate means based on observation_count\n",
    "    if premade_mapping is not None:\n",
    "        region_mapping = premade_mapping\n",
    "        df[region_col] = df[region_col].map(region_mapping)\n",
    "        return df, region_mapping\n",
    "\n",
    "    region_means = df.groupby(region_col)['observation_count'].mean().sort_values()\n",
    "    \n",
    "    # map to ordinal integers based on rank (0 = lowest avg count, 24 = highest)\n",
    "    region_mapping = {region: i for i, region in enumerate(region_means.index)}\n",
    "    df[region_col] = df[region_col].map(region_mapping)\n",
    "    \n",
    "    return df, region_mapping\n",
    "\n",
    "def get_reigon_mapping():\n",
    "    map ={'Dalsland': 0, 'Ångermanland': 1, 'Medelpad': 2, 'Jämtland': 3, 'Härjedalen': 4, 'Bohuslän': 5, 'Hälsingland': 6, 'Dalarna': 7, 'Norrbotten': 8, 'Blekinge': 9, 'Lappland': 10, 'Värmland': 11, 'Västmanland': 12, 'Gästrikland': 13, 'Småland': 14, 'Östergötland': 15, 'Närke': 16, 'Västerbotten': 17, 'Gotland': 18, 'Västergötland': 19, 'Halland': 20, 'Uppland': 21, 'Öland': 22, 'Södermanland': 23, 'Skåne': 24}\n",
    "    return map\n",
    "\n",
    "def drop_unused_columns(df:pd.DataFrame, drop_date:bool=True):\n",
    "    unused_cols = [\"observation_date\", \"time_observations_started\", \"weathercode\", \"obs_count_lag_1\", \"obs_count_lag_2\", \"obs_count_lag_3\", \"obs_count_lag_4\", \"obs_count_lag_5\"]\n",
    "    if not drop_date:\n",
    "        unused_cols.remove(\"observation_date\")\n",
    "    df = df.drop(columns=unused_cols)\n",
    "    return df\n",
    "\n",
    "def birdcount_binarization(df:pd.DataFrame):\n",
    "    df['bird_count_binary'] = (df['observation_count'] > 0).astype(int)\n",
    "    df = df.drop(columns=['observation_count'])\n",
    "    return df\n",
    "\n",
    "def split_data(df:pd.DataFrame, train_size:float=0.8, val_size:float=0.1, shuffle = True) -> dict[str, DataSet]:\n",
    "    # ------Helper function -----\n",
    "    def show_dataset_end_dates(data_dict: dict):\n",
    "        for bird, ds in data_dict.items():\n",
    "            print(f\"Bird: {bird}\")\n",
    "            # Check train, val, and test splits\n",
    "            for split_name in ['train', 'val', 'test']:\n",
    "                df = getattr(ds, split_name)\n",
    "                \n",
    "                if 'observation_date' in df.columns:\n",
    "                    end_val = int(df['observation_date'].max()) \n",
    "                elif 'year' in df.columns:\n",
    "                    # Fallback if date was dropped but year remains\n",
    "                    end_val = f\"Year: {df['year'].max()}\"\n",
    "                else:\n",
    "                    end_val = \"Date column not found\"\n",
    "                    \n",
    "                print(f\"  {split_name.capitalize()} End: {end_val}\")\n",
    "\n",
    "    # ---- Main function body ----            \n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    unique_birds=df[\"bird_type\"].unique()\n",
    "    # make dict with the keys train, val, test and empty dataframes as values\n",
    "    ret_dict = {}\n",
    "    for bird in unique_birds:\n",
    "        bird_df = df[df[\"bird_type\"]==bird]\n",
    "        bird_df = bird_df.drop(columns=[\"bird_type\"]).reset_index(drop=True)\n",
    "        n = bird_df.shape[0]\n",
    "        train_end = int(n * train_size)\n",
    "        val_end = int(n * (train_size + val_size))\n",
    "        \n",
    "        bird_df_train = bird_df.iloc[:train_end]\n",
    "        bird_df_val = bird_df.iloc[train_end:val_end]\n",
    "        bird_df_test = bird_df.iloc[val_end:]\n",
    "        \n",
    "        ret_dict[bird] = DataSet(train=bird_df_train, val=bird_df_val, test=bird_df_test)\n",
    "    if not shuffle:\n",
    "        print(\"Dataset end dates (no shuffle):\")\n",
    "        show_dataset_end_dates(ret_dict)\n",
    "    return ret_dict\n",
    "\n",
    "def drop_x_negative_samples(df, drop_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Here we downsample the negative samples (bird_count_binary == 0) by a given percentage.\n",
    "    This helps to balance the dataset, we have a large number of negative samples compared to positive ones.\n",
    "    \"\"\"\n",
    "    neg = df[(df['bird_count_binary'] == 0)]\n",
    "    pos = df[~((df['bird_count_binary'] == 0))]\n",
    "    \n",
    "    # Calculate number of samples to drop\n",
    "    num_to_drop = int(len(neg) * drop_percentage)\n",
    "    \n",
    "    # Randomly sample the indices to keep\n",
    "    neg_downsampled = neg.sample(n=len(neg) - num_to_drop, random_state=42)\n",
    "    \n",
    "    # Combine back and shuffle\n",
    "    new_df = pd.concat([pos, neg_downsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "def make_feature_compatible(df:pd.DataFrame, reference_df:pd.DataFrame):\n",
    "    # ensure the dataframe has all the feats and as many rows in the right order\n",
    "    cols_to_add =['sighted_lag_5', 'sighted_lag_3', 'sighted_lag_4', 'sighted_lag_2', 'bird_count_binary', 'sighted_lag_1']\n",
    "    for col in cols_to_add:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    # dupplicate all rows and have half of them bird_type = \"whteag\" and half = goleag\n",
    "    df = pd.concat([df.assign(bird_type='whteag'), df.assign(bird_type='goleag')], ignore_index=True)\n",
    "    # sort the colums like the reference df\n",
    "    df = df[reference_df.columns]\n",
    "    return df\n",
    "\n",
    "def fill_sighted_lag(filled: pd.DataFrame, original: pd.DataFrame):\n",
    "    filled['observation_date'] = pd.to_datetime(filled['observation_date'])\n",
    "    original['observation_date'] = pd.to_datetime(original['observation_date'])\n",
    "    \n",
    "    # Combine datasets to ensure rolling windows carry over from history to future\n",
    "    combined = pd.concat([original, filled]).drop_duplicates(['observation_date', 'bird_type', 'region'], keep='last')\n",
    "    combined = combined.sort_values(['bird_type', 'region', 'observation_date'])\n",
    "\n",
    "    # Shift bird_count_binary within each group to create lags\n",
    "    for i in range(1, 6):\n",
    "        combined[f'sighted_lag_{i}'] = combined.groupby(['bird_type', 'region'])['bird_count_binary'].shift(i)\n",
    "\n",
    "    # Update filled dataframe with calculated lags\n",
    "    filled = filled.drop(columns=[f'sighted_lag_{i}' for i in range(1, 6)])\n",
    "    filled = filled.merge(\n",
    "        combined[['observation_date', 'bird_type', 'region'] + [f'sighted_lag_{i}' for i in range(1, 6)]],\n",
    "        on=['observation_date', 'bird_type', 'region'],\n",
    "        how='left'\n",
    "    )\n",
    "    # convert sighted_lag columns to int\n",
    "    for i in range(1, 6):\n",
    "        filled[f'sighted_lag_{i}'] = filled[f'sighted_lag_{i}'].astype(int)\n",
    "    return filled\n",
    "\n",
    "def format_observation_date(df: pd.DataFrame):\n",
    "    df['observation_date'] = pd.to_datetime(df['observation_date']).dt.strftime('%Y-%m-%d') # Changed: Formatted date to y-m-d string\n",
    "    return df\n",
    "\n",
    "def divide_ds(ds:pd.DataFrame):\n",
    "    goleag_ds = ds[ds[\"bird_type\"]==\"goleag\"].drop(columns=[\"bird_type\"]).reset_index(drop=True)\n",
    "    whteag_ds = ds[ds[\"bird_type\"]==\"whteag\"].drop(columns=[\"bird_type\"]).reset_index(drop=True)\n",
    "    return goleag_ds, whteag_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c9cdf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (3.76s) \n",
      "Len hopsworks ds 273350\n",
      "Index(['region', 'observation_date', 'wind', 'rain', 'temperature',\n",
      "       'bird_type', 'year', 'month_1', 'month_2', 'month_3', 'month_4',\n",
      "       'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10',\n",
      "       'month_11', 'month_12', 'sighted_lag_1', 'sighted_lag_2',\n",
      "       'sighted_lag_3', 'sighted_lag_4', 'sighted_lag_5', 'bird_count_binary'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "hops_df = get_feature_group_as_df()\n",
    "print(\"Len hopsworks ds\",hops_df.shape[0])\n",
    "hops_df = true_false_to_01(hops_df)\n",
    "hops_df = sort_by_date(hops_df)\n",
    "hops_df, REGION_MAPPING = encode_reigon(hops_df)\n",
    "hops_df = drop_unused_columns(hops_df, drop_date = False)\n",
    "hops_df = birdcount_binarization(hops_df)\n",
    "print(hops_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0030a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 19)\n",
      "(350, 25)\n"
     ]
    }
   ],
   "source": [
    "weather_df=features()\n",
    "weather_df = to_hopsworks_df(weather_df)\n",
    "weather_df = sort_by_date(weather_df)\n",
    "REGION_MAP = get_reigon_mapping()\n",
    "weather_df, __ = encode_reigon(weather_df,premade_mapping =REGION_MAP)\n",
    "weather_df = true_false_to_01(weather_df)\n",
    "print(weather_df.shape)\n",
    "weather_df = make_feature_compatible(weather_df, reference_df=hops_df)\n",
    "print(weather_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69215ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 25) (175, 24) (175, 24)\n"
     ]
    }
   ],
   "source": [
    "mergd = fill_sighted_lag(weather_df,format_observation_date(hops_df))\n",
    "goleag_ds, whteag_ds = divide_ds(mergd)\n",
    "print(mergd.shape, goleag_ds.shape, whteag_ds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9367b9",
   "metadata": {},
   "source": [
    "## Infering the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a1dce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d261aa3d63a244a8823983bb80ca3096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/8209 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d026b5b2df4ebdbac37edc058fe64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/17425 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "# 2. Retrieve model metadata from Hopsworks (e.g., version 1)\n",
    "hops_goldag_meta = mr.get_model(\"Goleag_model\", version=1)\n",
    "hops_whteag_meta = mr.get_model(\"Whteag_model\", version=1)\n",
    "# 3. Download the model files to a local directory\n",
    "g_path = hops_goldag_meta.download()\n",
    "w_path = hops_whteag_meta.download()\n",
    "\n",
    "# 4. Re-initialize your model classes (ensure in_features matches your training data)\n",
    "goldag_model_loaded = BirdPercentModel(in_features=22, hidden_layers=[32, 16, 1])\n",
    "whteag_model_loaded = BirdPercentModel(in_features=22, hidden_layers=[64, 32, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861eaf33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef7c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_for_date(model: nn.Module, df: pd.DataFrame, date: int = 0) -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = df_to_tensor(df, date)\n",
    "        outputs = model(input_tensor).squeeze().numpy()\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "def df_to_tensor(df: pd.DataFrame, date: int = 0) -> torch.Tensor:\n",
    "    # Get the specific date value\n",
    "    unique_dates = df['observation_date'].unique()\n",
    "    target_date = unique_dates[date]\n",
    "    \n",
    "    # Filter and sort by region to ensure consistent row order\n",
    "    day_df = df[df['observation_date'] == target_date].sort_values('region')\n",
    "    \n",
    "    # Drop non-feature columns\n",
    "    # We use errors='ignore' in case columns were already removed\n",
    "    features = day_df.drop(columns=['observation_date', 'bird_count_binary', 'bird_type'], errors='ignore')\n",
    "    \n",
    "    return torch.tensor(features.values.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "def instert_update_rolling(df: pd.DataFrame, predictions: np.ndarray, date: int = 0) -> pd.DataFrame:\n",
    "    unique_dates = df['observation_date'].unique()\n",
    "    binary_preds = (predictions >= 0.5).astype(int)\n",
    "    \n",
    "    # Get regions for the current date in the same order as df_to_tensor\n",
    "    current_date = unique_dates[date]\n",
    "    current_day_mask = df['observation_date'] == current_date\n",
    "    regions = df[current_day_mask].sort_values('region')['region'].values\n",
    "    \n",
    "    # Map predictions to regions\n",
    "    pred_map = dict(zip(regions, binary_preds))\n",
    "\n",
    "    # Update current day bird_count_binary with the predictions\n",
    "    df.loc[current_day_mask, 'bird_count_binary'] = df.loc[current_day_mask, 'region'].map(pred_map)\n",
    "    \n",
    "    # Propagate to future lags\n",
    "    for i in range(1, 6):\n",
    "        if date + i < len(unique_dates):\n",
    "            future_date = unique_dates[date + i]\n",
    "            # Update each region's lag column for the future date\n",
    "            mask = df['observation_date'] == future_date\n",
    "            df.loc[mask, f'sighted_lag_{i}'] = df.loc[mask, 'region'].map(pred_map)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def predict(bird_df: pd.DataFrame, model: nn.Module) -> pd.DataFrame:\n",
    "    region_mapping = {0: 'Dalsland', 1: 'Ångermanland', 2: 'Medelpad', 3: 'Jämtland', 4: 'Härjedalen', 5: 'Bohuslän', 6: 'Hälsingland', 7: 'Dalarna', 8: 'Norrbotten', 9: 'Blekinge', 10: 'Lappland', 11: 'Värmland', 12: 'Västmanland', 13: 'Gästrikland', 14: 'Småland', 15: 'Östergötland', 16: 'Närke', 17: 'Västerbotten', 18: 'Gotland', 19: 'Västergötland', 20: 'Halland', 21: 'Uppland', 22: 'Öland', 23: 'Södermanland', 24: 'Skåne'}\n",
    "    \n",
    "    bird_df = bird_df.copy()\n",
    "    bird_df = bird_df.sort_values('observation_date')\n",
    "    unique_dates = bird_df['observation_date'].unique()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for date_idx in range(len(unique_dates)):\n",
    "        outputs = get_predictions_for_date(model, bird_df, date_idx)\n",
    "        \n",
    "        current_date = unique_dates[date_idx]\n",
    "        # Sorting by region ensures the n-th model output matches the n-th region ID\n",
    "        current_day_df = bird_df[bird_df['observation_date'] == current_date].sort_values('region')\n",
    "        region_ids = current_day_df['region'].values\n",
    "        \n",
    "        for r_id, prob in zip(region_ids, outputs):\n",
    "            results.append({\n",
    "                'observation_date': current_date,\n",
    "                'region': region_mapping.get(r_id, r_id), # Map ID to Name\n",
    "                'probability': float(prob)\n",
    "            })\n",
    "        \n",
    "        bird_df = instert_update_rolling(bird_df, outputs, date_idx)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "whteag_preds = predict(whteag_ds, whteag_model_loaded)\n",
    "whteag_preds[\"bird_type\"] = \"whteag\"\n",
    "goleag_preds = predict(goleag_ds, goldag_model_loaded)\n",
    "goleag_preds[\"bird_type\"] = \"goleag\"\n",
    "preds = pd.concat([whteag_preds, goleag_preds], ignore_index=True)\n",
    "preds = to_hopsworks_df(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd394299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation_date</th>\n",
       "      <th>region</th>\n",
       "      <th>probability</th>\n",
       "      <th>bird_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-21</td>\n",
       "      <td>Dalsland</td>\n",
       "      <td>0.472138</td>\n",
       "      <td>whteag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-21</td>\n",
       "      <td>Ångermanland</td>\n",
       "      <td>0.553263</td>\n",
       "      <td>whteag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-21</td>\n",
       "      <td>Medelpad</td>\n",
       "      <td>0.490655</td>\n",
       "      <td>whteag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-21</td>\n",
       "      <td>Jämtland</td>\n",
       "      <td>0.484350</td>\n",
       "      <td>whteag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-21</td>\n",
       "      <td>Härjedalen</td>\n",
       "      <td>0.514317</td>\n",
       "      <td>whteag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>2025-12-27</td>\n",
       "      <td>Halland</td>\n",
       "      <td>0.215116</td>\n",
       "      <td>goleag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2025-12-27</td>\n",
       "      <td>Uppland</td>\n",
       "      <td>0.191584</td>\n",
       "      <td>goleag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>2025-12-27</td>\n",
       "      <td>Öland</td>\n",
       "      <td>0.199252</td>\n",
       "      <td>goleag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>2025-12-27</td>\n",
       "      <td>Södermanland</td>\n",
       "      <td>0.186363</td>\n",
       "      <td>goleag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>2025-12-27</td>\n",
       "      <td>Skåne</td>\n",
       "      <td>0.193001</td>\n",
       "      <td>goleag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    observation_date        region  probability bird_type\n",
       "0         2025-12-21      Dalsland     0.472138    whteag\n",
       "1         2025-12-21  Ångermanland     0.553263    whteag\n",
       "2         2025-12-21      Medelpad     0.490655    whteag\n",
       "3         2025-12-21      Jämtland     0.484350    whteag\n",
       "4         2025-12-21    Härjedalen     0.514317    whteag\n",
       "..               ...           ...          ...       ...\n",
       "345       2025-12-27       Halland     0.215116    goleag\n",
       "346       2025-12-27       Uppland     0.191584    goleag\n",
       "347       2025-12-27         Öland     0.199252    goleag\n",
       "348       2025-12-27  Södermanland     0.186363    goleag\n",
       "349       2025-12-27         Skåne     0.193001    goleag\n",
       "\n",
       "[350 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c5c0221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1327256/fs/1315910/fg/1840748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 350/350 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: bird_sighting_predictions_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1327256/jobs/named/bird_sighting_predictions_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('bird_sighting_predictions_1_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for adding it to hopsworks feature store\n",
    "fs = project.get_feature_store()\n",
    "fg = fs.get_or_create_feature_group(\n",
    "    name=\"bird_sighting_predictions\",\n",
    "    version=1,\n",
    "    description=\"Predicted probabilities of bird sightings by region and date\",\n",
    "    primary_key=[\"observation_date\", \"region\", \"bird_type\"])\n",
    "\n",
    "fg.insert(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66659d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scalable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
